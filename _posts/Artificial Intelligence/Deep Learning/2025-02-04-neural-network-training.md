---
title: "[Deep Learning] 3. ì‹ ê²½ë§ í•™ìŠµ(Neural Network Training)ì— ëŒ€í•´ ì•Œì•„ë³´ì"
date: 2025-02-04 15:00:00 +0900
last_modified_at: 2025-02-04 15:00:00 +0900
categories: 
  - Deep Learning
tags:
  - deep learning
  - AI
  - deep learning
  - neural network
  - neural network training
  - loss function
  - gradient descent
  - backpropagation

toc: true
toc_sticky: true
---

# ğŸ¯ ì‹ ê²½ë§ í•™ìŠµ(Neural Network Training)

> ì‹ ê²½ë§ í•™ìŠµì— í•„ìš”í•œ ì§€ì‹ì¸ ì†ì‹¤ í•¨ìˆ˜, ìˆ˜ì¹˜ë¯¸ë¶„, ê¸°ìš¸ê¸°, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ë“±ì„ ê³µë¶€í•œë‹¤.

## ğŸ’¡ 1. Rule-based vs. Data-driven

- Rule-based í•™ìŠµ: ëª…ì‹œì ì¸ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê±°ë‚˜ ì˜ˆì¸¡í•˜ëŠ” ì ‘ê·¼ë²•
- Data-driven í•™ìŠµ: ëŒ€ëŸ‰ì˜ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ì ‘ê·¼ë²•

## ğŸš€ 2. ì†ì‹¤ í•¨ìˆ˜(Loss Function)

> ì‹ ê²½ë§ í•™ìŠµì—ì„œ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì‹ ê²½ë§ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

- ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤.
- ì†ì‹¤ í•¨ìˆ˜ì˜ ì¶œë ¥ì€ ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.

### ì™œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ”ê°€?

- ì‹ ê²½ë§ í•™ìŠµì—ì„œëŠ” ìµœì ì˜ ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜ì™€ í¸í–¥)ë¥¼ íƒìƒ‰í•  ë•Œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ëŠ¥í•œí•œ ì‘ê²Œí•˜ëŠ” ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì°¾ëŠ”ë‹¤.
- ì´ë•Œ ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ë¶„(ì •í™•íˆëŠ” ê¸°ìš¸ê¸°)ì„ ê³„ì‚°í•˜ê³ , ê·¸ ë¯¸ë¶„ê°’ì„ ë‹¨ì„œë¡œ ë§¤ê°œë³€ìˆ˜ì˜ ê°’ì„ ì„œì„œíˆ ê°±ì‹ í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.

#### ì˜ˆì‹œ

> ê°€ìƒì˜ ì‹ ê²½ë§ì´ ìˆê³ , ê·¸ ì‹ ê²½ë§ì˜ ì–´ëŠ í•œ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ê°€ ìˆì„ ë•Œ

1. ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ë¯¸ë¶„ -> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê°’ì„ ì•„ì£¼ ì¡°ê¸ˆ ë³€í™”ì‹œì¼°ì„ ë•Œ ì†ì‹¤ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ê°€
2. ë¯¸ë¶„ê°’ì´ ìŒìˆ˜ -> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì–‘ì˜ ë°©í–¥ìœ¼ë¡œ ë³€í™”ì‹œì¼œ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì„
3. ë¯¸ë¶„ê°’ì´ ì–‘ìˆ˜ -> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ìŒì˜ ë°©í–¥ìœ¼ë¡œ ë³€í™”ì‹œì¼œ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì„
4. ë¯¸ë¶„ê°’ì´ 0 -> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì–´ëŠ ìª½ìœ¼ë¡œ ì›€ì§ì—¬ë„ ì†ì‹¤ í•¨ìˆ˜ ê°’ì´ ì¤„ì–´ë“¤ì§€ ì•ŠìŒ
   - ë¯¸ë¶„ê°’ì´ 0ì´ ë  ë•Œ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê°±ì‹ ì´ ë©ˆì¶¤. í•™ìŠµ ì¢…ë£Œ

### ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢…ë¥˜

- í‰ê·  ì œê³± ì˜¤ì°¨(Mean Squared Error, MSE)
  - ì˜ˆì¸¡ê°’: $y_{i}$
  - ì‹¤ì œê°’: $t_{i}$

$$
E_1 = \frac{1}{n} \sum_{i=1}^{n} (y_{i} - t_{i})^2 \\

E_2 = \frac{1}{2} \sum_{i=1}^{n} (y_{i} - t_{i})^2
$$

- êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨(Cross-Entropy Error, CE)
  - ì •ë³´ì´ë¡ ì—ì„œ í™•ë¥ ë¶„í¬ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¬ëŠ” ë°©ë²•
  - ë°ì´í„°ê°€ ì‹ ê²½ë§ì„ ê±°ì³ ë‚˜ì˜¨ í™•ë¥  ë²¡í„°ì™€ ë¼ë²¨ë¡œ êµ¬í•œ êµì°¨ ì—”íŠ¸ë¡œí”¼

$$
E = -\frac{1}{n} \sum_{i=1}^{n} t_{i} \log y_{i}
$$

### DNNì˜ í•™ìŠµ - ìˆœì „íŒŒì™€ ì—­ì „íŒŒ

- ìˆœì „íŒŒ(Forward Propagation): : Input -> Output ë°©í–¥ìœ¼ë¡œ ì¶œë ¥ê°’ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •. ì‹¤ì¸¡ê°’ê³¼ ì°¨ì´ì¸ ì˜¤ì°¨(loss) ê³„ì‚°
- ì—­ì „íŒŒ(Backward Propagation): : Output -> Input ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•˜ëŠ” ê³¼ì •. ë”¥ëŸ¬ë‹ì—ì„œ í•™ìŠµì´ ì´ë£¨ì–´ì§

![Image](https://github.com/user-attachments/assets/54023e3f-cdf6-4fb0-bbdd-1b2beaa40d84){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: ìì²´ ì œì‘*

## ğŸ”§ 3. ìˆ˜ì¹˜ ë¯¸ë¶„(Numerical Differentiation)

> ìˆ˜ì¹˜ ë¯¸ë¶„ì€ ë¯¸ë¶„ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë‹¤.

- ì „ì§„ ì°¨ë¶„(Forward Difference)

> ì „ì§„ ì°¨ë¶„ì€ ì˜¤ì°¨ê°€ í¬ê³  ê³„ì‚°ëŸ‰ì´ ì ì–´ ëœ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.

$$
\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x)}{h}
$$

- ì¤‘ì‹¬ ì°¨ë¶„(Central Difference)

> ì¤‘ì‹¬ ì°¨ë¶„ì€ ì˜¤ì°¨ê°€ ì ê³  ê³„ì‚°ëŸ‰ì´ ë§ì•„ ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.

$$
\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h}
$$

- í›„ì§„ ì°¨ë¶„(Backward Difference)

> í›„ì§„ ì°¨ë¶„ì€ ì˜¤ì°¨ê°€ í¬ê³  ê³„ì‚°ëŸ‰ì´ ì ì–´ ëœ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.
$$
\frac{df(x)}{dx} \approx \frac{f(x) - f(x-h)}{h}
$$


### [ì°¸ê³ ] í…Œì¼ëŸ¬ ê¸‰ìˆ˜ ì „ê°œë¥¼ ì‚¬ìš©í•œ ì˜¤ì°¨ ë¶„ì„

> ì™œ ì¤‘ì‹¬ ì°¨ë¶„ì„ ì‚¬ìš©í•˜ëŠ”ê°€?

$$
f(x + h) = f(x) + f'(x)h + \frac{f''(x)}{2!}h^2 + \frac{f'''(x)}{3!}h^3 + O(h^4)
$$

- ì „ì§„ ì°¨ë¶„ì˜ ì˜¤ì°¨
  - ê²°ê³¼ì ìœ¼ë¡œ $O(h)$ì˜ ì˜¤ì°¨ë¥¼ ê°€ì§

$$
\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x)}{h} = f'(x) + \frac{f''(x)}{2}h + \frac{f'''(x)}{6}h^2 + O(h^3)
$$

- ì¤‘ì‹¬ ì°¨ë¶„ì˜ ì˜¤ì°¨
  - ê²°ê³¼ì ìœ¼ë¡œ $O(h^2)$ì˜ ì˜¤ì°¨ë¥¼ ê°€ì§
$$
\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h} = f'(x) + \frac{f'''(x)}{6}h^2 + O(h^4)
$$

- í›„ì§„ ì°¨ë¶„ì˜ ì˜¤ì°¨
  - ê²°ê³¼ì ìœ¼ë¡œ $O(h)$ì˜ ì˜¤ì°¨ë¥¼ ê°€ì§

$$
\frac{df(x)}{dx} \approx \frac{f(x) - f(x-h)}{h} = f'(x) - \frac{f''(x)}{2}h + \frac{f'''(x)}{6}h^2 + O(h^3)
$$

## ğŸ“ 4. ê¸°ìš¸ê¸°(Gradient)

### ì¼ë³€ìˆ˜ ë¯¸ë¶„ vs ë‹¤ë³€ìˆ˜ ë¯¸ë¶„

- ì¼ë³€ìˆ˜ ë¯¸ë¶„

$$
\frac{df(x)}{dx} \approx \frac{f(x+h) - f(x-h)}{2h}
$$

![Image](https://github.com/user-attachments/assets/7bd42f00-ce03-47d5-964a-9be85fc5ff27){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250204)*

- ë‹¤ë³€ìˆ˜ ë¯¸ë¶„

$$
\frac{df(X)}{dX} \approx \frac{f(X + h\vec{v}) - f(X-h\vec{v})}{2h}
$$

![Image](https://github.com/user-attachments/assets/33fbc825-95e7-4f50-bf51-99f4b8ccbf50){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250204)*

### ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent)

- ì—­ì „íŒŒëŠ” ì¶œë ¥ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ì¸ ì˜¤ì°¨ê°€ ìµœì†Œê°€ ë˜ë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•¨.
  - ì´ë•Œ ê°€ì¤‘ì¹˜ ê°±ì‹  ë°©ë²•ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•¨
- ì˜¤ì°¨ê°€ ë‚®ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•  ëª©ì ìœ¼ë¡œ ë¯¸ë¶„
- **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**: ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ $X_{n+1}$ ì—…ë°ì´íŠ¸
  - $\eta$ëŠ” í•™ìŠµë¥ (Learning Rate)ë¡œ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜
  - $\nabla f(X_{n})$ëŠ” ê¸°ìš¸ê¸°(Gradient)ë¡œ ì˜¤ì°¨ê°€ ë‚®ì•„ì§€ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚´

$$
X_{n+1} = X_{n} - \eta \nabla f(X_{n})
$$

![Image](https://github.com/user-attachments/assets/4305f37a-7081-4f8d-879d-50e79327d26d){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)*

#### ì™œ $X_n$ì˜ ìµœì ê°’(ìµœì†Œê°’)ì„ ì°¾ì•„ì•¼ í•˜ëŠ”ê°€?

> $X_n$ì˜ ìµœì ê°’ì„ ì°¾ëŠ” ê²ƒì€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì—¬ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

- ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ìµœì ê°’ì„ ì°¾ì•„ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´(ì†ì‹¤ í•¨ìˆ˜)ê°€ ìµœì†Œê°€ ë˜ë„ë¡ í•˜ê¸° ìœ„í•´ì„œì´ë‹¤.
- ìœ„ì˜ ê²½ì‚¬í•˜ê°•ë²• ì‹ì—ì„œ $X$ëŠ” ì•„ë˜ ì†ì‹¤ í•¨ìˆ˜ì— ìˆëŠ” ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë§¤ê°œë³€ìˆ˜ì´ë‹¤.
- ìœ„ì˜ ê²½ì‚¬í•˜ê°•ë²• ì‹ì—ì„œ $f(X)$ëŠ” ì†ì‹¤ í•¨ìˆ˜ $E(w, b)$ì´ê³  $\nabla f(X)$ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ì´ë‹¤.
  - **ì†ì‹¤ í•¨ìˆ˜ ê¸°ìš¸ê¸°ì™€ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸ í•˜ë©´ ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë˜ëŠ” ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.**

$$
E(w, b) = \frac{1}{2} \sum_{i=1}^{n} (y_{i} - t_{i})^2
$$

## ğŸ§  5. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°

### í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ 4ë‹¨ê³„

1. ì „ì œ: ì‹ ê²½ë§ì—ëŠ” ì ìš© ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ **í•™ìŠµ**ì´ë¼ê³  í•œë‹¤.
2. Step1 - ë¯¸ë‹ˆë°°ì¹˜: í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤. ì„ ë³„ëœ ë°ì´í„°ë¥¼ **ë¯¸ë‹ˆë°°ì¹˜(Mini-batch)**ë¼ê³  í•œë‹¤.
3. Step2 - ê¸°ìš¸ê¸° ì‚°ì¶œ: ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„°ë¥¼ í†µí•´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì‚°ì¶œí•œë‹¤.
4. Step3 - ê°€ì¤‘ì¹˜ ë° í¸í–¥ ê°±ì‹ : ê¸°ìš¸ê¸°ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê°±ì‹ í•œë‹¤.
5. ë°˜ë³µ: ìœ„ì˜ ê³¼ì •(Step1~3)ì„ ë°˜ë³µí•œë‹¤.

### ë”¥ëŸ¬ë‹ì˜ í•™ìŠµ ë‹¨ìœ„: Batchì™€ Epoch

- ë°°ì¹˜(Batch)
  - ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ ë³„í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë§Œë“œëŠ” ê²ƒì„ ë°°ì¹˜ë¼ê³  í•œë‹¤.
- ì—í­(Epoch)
  - ì „ì²´ ë°ì´í„°ì…‹ì„ ëª¨ë‘ í•™ìŠµí•˜ëŠ” ê³¼ì •(ìˆœì „íŒŒ -> ì—­ì „íŒŒ -> ê°€ì¤‘ì¹˜ ë° í¸í–¥ ê°±ì‹ )ì„ í•œ ë²ˆ ì§„í–‰í•œ ê²ƒì„ í•œ ì—í­ì´ë¼ê³  í•œë‹¤.
  - ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•´ì•¼ ê²½ì‚¬í•˜ê°•ë²•ì˜ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŒ
  - ì—í­ì´ í¬ë©´ overfitting ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆê³  ì—í­ì´ ì‘ìœ¼ë©´ underfitting ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
- ë°˜ë³µ(Iteration)
  - ì—í­ì„ ë‚˜ëˆ„ì–´ ì‹¤í–‰í•˜ëŠ” íšŸìˆ˜(ë¯¸ë‹ˆë°°ì¹˜ì˜ ê°œìˆ˜)

![Image](https://github.com/user-attachments/assets/61edaf30-e566-448f-848e-54a7680fc267){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: ìì²´ ì œì‘*

### ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ : Stochastic Gradient Descent

- í›ˆë ¨ ë°ì´í„° ì¤‘ ë¬´ì‘ìœ„ë¡œ ì„ ë³„í•œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆ ë°°ì¹˜ë¼ê³  í•¨.
- íš¨ìœ¨ì ì¸ ê³„ì‚°, ìˆ˜ë ´ ì†ë„ ê°œì„ , í•™ìŠµì†ë„ ê°œì„ 

![Image](https://github.com/user-attachments/assets/b2c5ff2e-b0de-4a99-8fcf-bead55e721bd){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [EfficientDL: Mini-batch Gradient Descent Explained](https://statusneo.com/efficientdl-mini-batch-gradient-descent-explained/)*

