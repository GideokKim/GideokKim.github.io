---
title: "[Deep Learning] 4. ì˜¤ì°¨ ì—­ì „íŒŒ(Error Backpropagation)ì— ëŒ€í•´ ì•Œì•„ë³´ì"
date: 2025-02-05 15:00:00 +0900
last_modified_at: 2025-02-05 15:00:00 +0900
categories: 
  - Deep Learning
tags:
  - deep learning
  - AI
  - neural network
  - neural network training
  - loss function
  - gradient descent
  - backpropagation
  - error backward propagation

toc: true
toc_sticky: true
---

# ğŸ¯ ì˜¤ì°¨ ì—­ì „íŒŒ(Error Backpropagation)

> ì‹ ê²½ë§ í•™ìŠµì— í•„ìš”í•œ ì§€ì‹ì¸ ì†ì‹¤ í•¨ìˆ˜, ìˆ˜ì¹˜ë¯¸ë¶„, ê¸°ìš¸ê¸°, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ë“±ì„ ê³µë¶€í•œë‹¤.

## 1. ğŸ“Š ê³„ì‚° ê·¸ë˜í”„(Computation Graph)

### ìˆœì „íŒŒ(Forward Propagation)

> ì‚¬ê³¼ì˜ ê°œë‹¹ ê°€ê²©(x), ì‚¬ê³¼ì˜ ê°œìˆ˜(m), ì†Œë¹„ì„¸(t)ì¼ ë•Œ ì´ê¸ˆì•¡ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì˜ˆì‹œë¡œ ë³´ì. ì—¬ê¸°ì„œ ë‚´ê°€ ì˜ˆì¸¡í•œ ê°œë‹¹ ê°€ê²©ì€ 100ì›, ê°œìˆ˜ëŠ” 2ê°œ, ì†Œë¹„ì„¸ëŠ” 10%ë¡œ ìƒê°í•˜ì.

- ì†Œë¹„ì„¸ ë¶™ê¸° ì „ ê¸ˆì•¡ í•¨ìˆ˜: $z(x, \ m) = x \cdot m$
- ì´ê¸ˆì•¡ í•¨ìˆ˜: $f(x, \ m, \ t) = x \cdot m \cdot t$
- ì´ê¸ˆì•¡ í•¨ìˆ˜ì˜ ê° ë³€ìˆ˜ë³„ í¸ë¯¸ë¶„ ê°’ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
\frac{\partial f(x, m, t)}{\partial x} = m \cdot t \\[1em]
\frac{\partial f(x, m, t)}{\partial m} = x \cdot t \\[1em]
\frac{\partial f(x, m, t)}{\partial t} = x \cdot m
$$

- ê³„ì‚° ê·¸ë˜í”„ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

![Image](https://github.com/user-attachments/assets/b307a9bf-3b97-4295-98e4-0ac8a562e8d6){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: ìì²´ ì œì‘*

### ì—­ì „íŒŒ(Backward Propagation)

> ì‹¤ì œ ì´ê¸ˆì•¡ì´ 219ì›ì¼ ë•Œ, ì˜ˆì¸¡ê°’(220ì› = 100ì› Ã— 2ê°œ Ã— 1.1)ê³¼ì˜ ì°¨ì´ë¥¼ í†µí•´ ê° ë³€ìˆ˜ë“¤ì´ ë³€í•˜ë©´ ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ì—­ìœ¼ë¡œ ê³„ì‚°í•´ë³´ì.

1. **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜**
   - $L = \frac{1}{2}(y_{pred} - y_{true})^2$
   - $L = \frac{1}{2}(220 - 219)^2 = 0.5$

2. **ì—­ì „íŒŒ ê³„ì‚°**
   - $\frac{\partial L}{\partial f} = (y_{pred} - y_{true}) = 1$ (ì˜¤ì°¨)
   
   ê° ë³€ìˆ˜ë³„ ê¸°ìš¸ê¸°:
   - $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial f} \cdot \frac{\partial f}{\partial z} \cdot \frac{\partial z}{\partial x} = 1 \cdot (2 \cdot 1.1) = 2.2$ 
   - $\frac{\partial L}{\partial m} = \frac{\partial L}{\partial f} \cdot \frac{\partial f}{\partial z} \cdot \frac{\partial z}{\partial m} = 1 \cdot (100 \cdot 1.1) = 110$
   - $\frac{\partial L}{\partial t} = \frac{\partial L}{\partial f} \cdot \frac{\partial f}{\partial z} \cdot \frac{\partial z}{\partial t} = 1 \cdot (100 \cdot 2) = 200$

3. **í•´ì„**
   - ê°€ê²©(x)ì´ 1ì› ì¦ê°€í•  ë•Œ:
     * í˜„ì¬: $f(100, 2, 1.1) = 220$
     * ë³€ê²½: $f(101, 2, 1.1) = 222.2$ (**ì˜ˆì¸¡ê°’ 2.2 ì¦ê°€**)
     * ìƒˆë¡œìš´ ì†ì‹¤: $\frac{1}{2}(222.2 - 219)^2 = 5.12$
     * ì†ì‹¤ ì¦ê°€ëŸ‰: $5.12 - 0.5 = 4.62$

   - ê°œìˆ˜(m)ê°€ 1ê°œ ì¦ê°€í•  ë•Œ:
     * í˜„ì¬: $f(100, 2, 1.1) = 220$
     * ë³€ê²½: $f(100, 3, 1.1) = 330$ (**ì˜ˆì¸¡ê°’ 110 ì¦ê°€**)
     * ìƒˆë¡œìš´ ì†ì‹¤: $\frac{1}{2}(330 - 219)^2 = 6,160.5$
     * ì†ì‹¤ ì¦ê°€ëŸ‰: $6,160.5 - 0.5 = 6,160$

   - ì†Œë¹„ì„¸(t)ê°€ 1% ì¦ê°€í•  ë•Œ:
     * í˜„ì¬: $f(100, 2, 1.1) = 220$
     * ë³€ê²½: $f(100, 2, 1.11) = 222$ (**ì˜ˆì¸¡ê°’ 2 ì¦ê°€**)
     * ìƒˆë¡œìš´ ì†ì‹¤: $\frac{1}{2}(222 - 219)^2 = 4.5$
     * ì†ì‹¤ ì¦ê°€ëŸ‰: $4.5 - 0.5 = 4$

   - ê²°ë¡ : 
     * ì˜ˆì¸¡ê°’(220ì›)ì´ ì‹¤ì œê°’(219ì›)ê³¼ ë§¤ìš° ê°€ê¹Œì›Œ ì´ˆê¸° ì†ì‹¤ì´ ì‘ìŒ
     * ê°€ê²©(x)ê³¼ ì†Œë¹„ì„¸(t)ì˜ ì‘ì€ ë³€í™”ëŠ” ì†ì‹¤ì— ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ì˜í–¥ì„ ë¯¸ì¹¨
     * ê°œìˆ˜(m)ì˜ ë³€í™”ëŠ” ì†ì‹¤ì„ í¬ê²Œ ì¦ê°€ì‹œí‚´
     * í˜„ì¬ ìƒíƒœê°€ ì´ë¯¸ ìµœì ì— ê°€ê¹ê¸° ë•Œë¬¸ì— í° ë³€í™”ëŠ” ì˜¤íˆë ¤ ì†ì‹¤ì„ ì¦ê°€ì‹œí‚´

![Image](https://github.com/user-attachments/assets/d8c5b5a1-d84a-4059-b851-d47293912c05){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: ìì²´ ì œì‘*

#### ì—­ì „íŒŒ ìƒì„¸ ê³¼ì •

1. **ê³„ì‚° ê·¸ë˜í”„ êµ¬ì¡°**
   ```
   x(100ì›) â†’ [Ã—] â†’ [Ã—] â†’ f(220ì›)
              â†‘      â†‘
           m(2ê°œ)  t(1.1)
   ```

2. **ê° ê³±ì…ˆ ê²Œì´íŠ¸ë³„ ì—­ì „íŒŒ ê³¼ì •**

   a) ë§ˆì§€ë§‰ ê³±ì…ˆ ê²Œì´íŠ¸ (z Ã— t = f)
   ```python
   # ì…ë ¥: z = 200, t = 1.1
   # ìƒìœ„ì—ì„œ ì „ë‹¬ëœ ê¸°ìš¸ê¸°: 1 (ì˜ˆì¸¡ê°’ 220ì›ê³¼ ì‹¤ì œê°’ 219ì›ì˜ ì°¨ì´)
   
   âˆ‚t = 1 Ã— z = 1 Ã— 200 = 200    # të¡œ í–¥í•˜ëŠ” ê¸°ìš¸ê¸°
   âˆ‚z = 1 Ã— t = 1 Ã— 1.1 = 1.1    # zë¡œ í–¥í•˜ëŠ” ê¸°ìš¸ê¸°
   ```

   b) ì²« ë²ˆì§¸ ê³±ì…ˆ ê²Œì´íŠ¸ (x Ã— m = z)
   ```python
   # ì…ë ¥: x = 100, m = 2
   # ìƒìœ„ì—ì„œ ì „ë‹¬ëœ ê¸°ìš¸ê¸°: 1.1
   
   âˆ‚x = 1.1 Ã— m = 1.1 Ã— 2 = 2.2      # xë¡œ í–¥í•˜ëŠ” ê¸°ìš¸ê¸°
   âˆ‚m = 1.1 Ã— x = 1.1 Ã— 100 = 110    # mìœ¼ë¡œ í–¥í•˜ëŠ” ê¸°ìš¸ê¸°
   ```

3. **ìµœì¢… í¸ë¯¸ë¶„ ê°’ ê²€ì¦**
   ```python
   # xì— ëŒ€í•œ í¸ë¯¸ë¶„
   âˆ‚f/âˆ‚x = m Ã— t = 2 Ã— 1.1 = 2.2
   # ì—­ì „íŒŒ ê²°ê³¼: dx = 2.2
   
   # mì— ëŒ€í•œ í¸ë¯¸ë¶„
   âˆ‚f/âˆ‚m = x Ã— t = 100 Ã— 1.1 = 110
   # ì—­ì „íŒŒ ê²°ê³¼: dm = 110
   
   # tì— ëŒ€í•œ í¸ë¯¸ë¶„
   âˆ‚f/âˆ‚t = x Ã— m = 100 Ã— 2 = 200
   # ì—­ì „íŒŒ ê²°ê³¼: dt = 200
   ```

4. **ì—­ì „íŒŒì˜ ì˜ë¯¸**
   - ê° ë³€ìˆ˜ê°€ ìµœì¢… ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³„ì‚°
   - ì–‘ìˆ˜ ê¸°ìš¸ê¸°: í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì¦ê°€ì‹œí‚¤ë©´ ì†ì‹¤ì´ ì¦ê°€
   - ê¸°ìš¸ê¸°ì˜ ì ˆëŒ“ê°’: ë³€ìˆ˜ ë³€í™”ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ í¬ê¸°
     - ì˜ˆ: mì˜ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ“ê°’(110)ì´ xì˜ ê¸°ìš¸ê¸°ì˜ ì ˆëŒ“ê°’(2.2)ë³´ë‹¤ í¬ë¯€ë¡œ, mì„ ì¡°ì •í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì 

### ë§ì…ˆ ê²Œì´íŠ¸ì™€ ê³±ì…ˆ ê²Œì´íŠ¸ì˜ ì—­ì „íŒŒ ê³¼ì •

- ë§ì…ˆ ê²Œì´íŠ¸ì˜ ì—­ì „íŒŒ: ë§ì…ˆ ê²Œì´íŠ¸ëŠ” ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•œë‹¤.

- ê³±ì…ˆ ê²Œì´íŠ¸ì˜ ì—­ì „íŒŒ: ê³±ì…ˆ ê²Œì´íŠ¸ëŠ” ìˆœì „íŒŒ ë•Œ ì…ë ¥ê°’ì„ ì„œë¡œ ë°”ê¾¼ ê°’ì„ ê³±í•˜ì—¬ ì „ë‹¬í•œë‹¤.(chain rule)

### ê° ê²Œì´íŠ¸ë³„ ìˆœì „íŒŒ ë° ì—­ì „íŒŒ íŒŒì´ì¬ ì½”ë“œ

#### ë§ì…ˆ ê²Œì´íŠ¸

```python
class AddLayer:
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y

        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1

        return dx, dy
```

#### ê³±ì…ˆ ê²Œì´íŠ¸

```python
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    def forward(self, x, y):
        self.x = x
        self.y = y                
        out = x * y

        return out

    def backward(self, dout):
        dx = dout * self.y  # xì™€ yë¥¼ ë°”ê¾¼ë‹¤.
        dy = dout * self.x

        return dx, dy
```

## 2. ğŸ”§ Affine ê³„ì¸µ

> Affine ê³„ì¸µì€ ì…ë ¥ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±(ì„ í˜• ë³€í™˜)í•˜ê³  í¸í–¥ì„ ë”í•˜ëŠ” ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê³„ì¸µì´ë‹¤. ì´ë ‡ê²Œ ì„ í˜• ë³€í™˜(Linear Transformation) í›„ í‰í–‰ ì´ë™(Translation)í•˜ëŠ” ê²ƒì„ Affine transformationì´ë¼ê³  í•œë‹¤. ìˆ˜í•™ì  ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

$$
Y = X \cdot W + B
$$

- ì—¬ê¸°ì„œ $X$ëŠ” ì…ë ¥ ë°ì´í„°, $W$ëŠ” ê°€ì¤‘ì¹˜, $B$ëŠ” í¸í–¥, $Y$ëŠ” ì¶œë ¥ ë°ì´í„°ì´ë‹¤.
- ë‹¨ìˆœíˆ ìƒê°í•˜ë©´ ì•ì—ì„œ ì—´ì‹¬íˆ ì„¤ëª…í•œ ê²ƒë“¤ì˜ **í–‰ë ¬ ë²„ì „**ì´ë‹¤.
  - ë°°ì¹˜ ì²˜ë¦¬ê°€ ì—†ìœ¼ë©´ $W$ë§Œ í–‰ë ¬ì´ê³  ë‚˜ë¨¸ì§„ ë²¡í„°ì´ë‹¤.
  - ë°°ì¹˜ ì²˜ë¦¬ê¹Œì§€ ë”í•´ì§€ë©´ ëª¨ë“  ë³€ìˆ˜ê°€ í–‰ë ¬ì´ ë˜ì–´ ì™„ì „íˆ í–‰ë ¬ ê³±ì…ˆìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.
- ìˆ˜í•™ì ìœ¼ë¡œ ì—­ì „íŒŒë„ í–‰ë ¬ ë²„ì „ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤.

## 3. ğŸ”§ í™œì„±í™” í•¨ìˆ˜(Activation Function)ì˜ ì—­ì „íŒŒ


$$
\frac{dL}{dw} = \frac{dL}{dy} \cdot \frac{dy}{dz} \cdot \frac{dz}{dw} \\
$$
$$
\frac{dL}{dy}: gradient \ of \ loss \ function\\
$$
$$
\frac{dy}{dz}: gradient \ of \ activation \ function\\
$$
$$
\frac{dz}{dw}: gradient \ of \ weight
$$


### Sigmoid í•¨ìˆ˜ì˜ ì—­ì „íŒŒ

> sigmoid í•¨ìˆ˜ëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ë¯¸ë¶„ ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$
\frac{d}{dx}sigmoid(x) = sigmoid(x)\{1-sigmoid(x)\}
$$

- ì—¬ê¸°ì„œ $x$ëŠ” ì…ë ¥ ë°ì´í„°, $sigmoid(x)$ëŠ” sigmoid í•¨ìˆ˜ì˜ ì¶œë ¥ ë°ì´í„°ì´ë‹¤.
- ì—­ì „íŒŒ ê³¼ì •ì—ì„œ í™œì„±í™” í•¨ìˆ˜ ê²Œì´íŠ¸ë¥¼ ë„˜ì–´ê°ˆ ë•Œ ê¸°ìš¸ê¸°ê°€ ê³±í•´ì§„ë‹¤.

#### Sigmoid í•¨ìˆ˜ì˜ ë¬¸ì œì 

- sigmoid í•¨ìˆ˜ì˜ ëª¨ì–‘ì„ ë³´ë©´ ì–‘ì˜ ë¬´í•œëŒ€ì™€ ìŒì˜ ë¬´í•œëŒ€ë¡œ ê°ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.
- ì´ë¡œ ì¸í•´ ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •ì¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ê°€ ë” ì´ìƒ ì§„í–‰ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ReLU í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.

### ReLU í•¨ìˆ˜ì˜ ì—­ì „íŒŒ

> ReLU í•¨ìˆ˜ëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ë¯¸ë¶„ ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$
\frac{d}{dx}ReLU(x) = \begin{cases} 
1 & \text{if } x > 0 \\
0 & \text{if } x \leq 0
\end{cases}
$$


### Softmaxë¥¼ ì´ìš©í•œ ì‹ ê²½ë§ ì¶”ë¡  ê³¼ì •

> Softmax with Loss ê³„ì¸µì€ ì¶œë ¥ì¸µì—ì„œ ì‚¬ìš©ë˜ëŠ” í™œì„±í™” í•¨ìˆ˜ì´ë‹¤. Softmax í•¨ìˆ˜ì— ì†ì‹¤ í•¨ìˆ˜ì¸ Cross Entropy Errorë¥¼ í¬í•¨í•˜ì—¬ êµ¬í˜„í•œë‹¤.

<center>
<img width="1024" alt="Image" src="https://github.com/user-attachments/assets/cd7d96bf-c006-47dc-9f2f-517e93f0ce61" />
</center>

*ì´ë¯¸ì§€ ì¶œì²˜: [ã€ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1(ë¦¬ë§ˆìŠ¤í„°íŒ)ã€](https://github.com/WegraLee/deep-learning-from-scratch)*

- Softmax-with-Loss ë¯¸ë¶„
  - ì—¬ê¸°ì„œ $y_1, y_2, y_3$ëŠ” ì¶œë ¥ì¸µì˜ ì¶œë ¥ ë°ì´í„°, $t_1, t_2, t_3$ëŠ” ì‹¤ì œ ë°ì´í„°ì´ë‹¤.

$$
\frac{\partial L}{\partial a} = (y_1 - t_1, \ y_2 - t_2, \ y_3 - t_3)
$$

- Softmax-with-Loss ê³„ì¸µì˜ ì—­ì „íŒŒ ê³¼ì •

<center>
<img width="1024" alt="Image" src="https://github.com/user-attachments/assets/fc8181d7-4d07-4504-aadc-c734084814b9" />
</center>

*ì´ë¯¸ì§€ ì¶œì²˜: [ã€ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1(ë¦¬ë§ˆìŠ¤í„°íŒ)ã€](https://github.com/WegraLee/deep-learning-from-scratch)*

## 4. ğŸ§  ì˜¤ì°¨ ì—­ì „íŒŒ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°

### í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ 4ë‹¨ê³„

1. ì „ì œ: ì‹ ê²½ë§ì—ëŠ” ì ìš© ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ **í•™ìŠµ**ì´ë¼ê³  í•œë‹¤.
2. Step1 - ë¯¸ë‹ˆë°°ì¹˜: í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤. ì„ ë³„ëœ ë°ì´í„°ë¥¼ **ë¯¸ë‹ˆë°°ì¹˜(Mini-batch)**ë¼ê³  í•œë‹¤.
3. Step2 - ê¸°ìš¸ê¸° ì‚°ì¶œ: ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ê¸°ìš¸ê¸°ë¥¼ ì‚°ì¶œí•œë‹¤. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚¤ëŠ” ë²¡í„°ì´ë‹¤.
4. Step3 - ê°€ì¤‘ì¹˜ ë° í¸í–¥ ê°±ì‹ : ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ í•œë‹¤.
5. ë°˜ë³µ: ìœ„ì˜ ê³¼ì •(Step1~3)ì„ ë°˜ë³µí•œë‹¤.

### êµ¬í˜„ ì½”ë“œ

> [ì½”ë“œ ì¶œì²˜: ã€ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹ 1(ë¦¬ë§ˆìŠ¤í„°íŒ)ã€](https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch05/two_layer_net.py)

```python
# coding: utf-8
import sys, os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
import numpy as np
from common.layers import *
from common.gradient import numerical_gradient
from collections import OrderedDict


class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) 
        self.params['b2'] = np.zeros(output_size)

        # ê³„ì¸µ ìƒì„±
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()
        
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
        
        return x
        
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # ê²°ê³¼ ì €ì¥
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads
```
