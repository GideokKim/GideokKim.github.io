---
title: "[Deep Learning] 5. ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•"
date: 2025-02-05 22:00:00 +0900
last_modified_at: 2025-02-05 22:00:00 +0900
categories: 
  - Deep Learning
tags:
  - deep learning
  - AI
  - neural network
  - optimizer
  - gradient descent
  - stochastic gradient descent

toc: true
toc_sticky: true
---

# ğŸ¯ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•

> ë‹¤ì–‘í•œ optimizerë¥¼ ì‚¬ìš©í•´ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°±ì‹ í•´ë³´ì. í•™ìŠµë¥ ê³¼ ê¸°ìš¸ê¸° ê²°ì • ë°©ë²•ì— ë”°ë¼ ë‹¤ì–‘í•œ ë°©ë²•ì´ ìˆë‹¤.

- í˜„ì¬ Adam Optimizerê°€ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆë‹¤.
- ëª¨ë“  optimizerê°€ ê²°êµ­ ì‹œê°„ì´ ì§€ë‚˜ë©´ ì „ë¶€ ìˆ˜ë ´í•˜ì§€ë§Œ ìˆ˜ë ´ ì†ë„ê°€ ë‹¬ë¼ì„œ ëŒ€í˜• ëª¨ë¸ì˜ ê²½ìš° ì´ ì°¨ì´ë¡œ ë¹„ìš© ì°¨ì´ê°€ ì»¤ì§„ë‹¤.
- ê·¸ëŸ¬ë¯€ë¡œ í˜„ì—…ì—ì„œëŠ” optimizerì˜ ì„±ëŠ¥ë„ ë¬´ì‹œí•  ìˆ˜ ì—†ë‹¤.

![Image](https://github.com/user-attachments/assets/8d2aa263-b68c-41a0-800e-aebd497a52e6){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: ìì²´ ì œì‘*

## 1. ğŸ“ˆ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)

### ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²•

> ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²•: **ëª¨ë“  ë°ì´í„°ë¥¼ ëŒ€ìƒ**ìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.

![Image](https://github.com/user-attachments/assets/c7568ce5-5c99-4e53-96fb-0ec866e83a23){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

### í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent)

> í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•: **ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ**í•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.

- ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²•ê³¼ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì˜ ì°¨ì´ëŠ” ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ëŒ€ìƒì´ ë‹¤ë¥´ë‹¤ëŠ” ì ì´ë‹¤.
- ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²•ì€ ëª¨ë“  ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì§€ë§Œ, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì€ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•œë‹¤.
- í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•ì€ ê¸°ë³¸ ê²½ì‚¬ í•˜ê°•ë²•ì— ë¹„í•´ ê³„ì‚° ì‹œê°„ì´ ì ê²Œ ê±¸ë¦¬ê³ , ë” ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
- **ìˆ˜ë ´ ë°©í–¥ì´ í”ë“¤ë¦¬ëŠ” ë¬¸ì œ(ë¹„íš¨ìœ¨ì ì¸ íƒìƒ‰ ê²½ë¡œ ìœ ë°œ)ê°€ ìˆë‹¤.**
- **í•™ìŠµë¥ ì˜ ì´ˆê¸°ê°’ì— ë”°ë¼ ì„±ëŠ¥ í¸ì°¨ê°€ í¬ë‹¤.**

$$
W_{t+1} = W_t - \eta \frac{\partial L}{\partial W}
$$

- $W_{t+1}$: ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜
- $W_t$: í˜„ì¬ ë§¤ê°œë³€ìˆ˜
- $\eta$: í•™ìŠµë¥ 
- $\frac{\partial L}{\partial W}$: ê¸°ìš¸ê¸°

![Image](https://github.com/user-attachments/assets/c17b5c32-e0eb-49c1-a274-dd13357cd550){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

## 2. ğŸ’ª ëª¨ë©˜í…€(Momentum)

> ëª¨ë©˜í…€: ì´ì „ ê¸°ìš¸ê¸°ì˜ ì˜í–¥ì„ ë°˜ì˜í•œë‹¤. Gradient Descentì— í˜„ì¬ ê´€ì„±ì„ ì¶”ê°€í•œë‹¤.

- **ê·¹ì†Œì ì— ë¹ ì§€ë”ë¼ë„ ê´€ì„±ì˜ í˜ìœ¼ë¡œ íƒˆì¶œí•  ê°€ëŠ¥ì„±ì´ ìƒê¹€**

$$
v = \alpha v - \eta \frac{\partial L(W)}{\partial W}
$$

- $v$: ì´ë™ ì†ë„
- $\alpha$: ê´€ì„± ê³„ìˆ˜(ë°˜ì˜ë¥ )
- $\eta$: í•™ìŠµë¥ 
- $\nabla E(W)$: ê¸°ìš¸ê¸°

$$
W_{t+1} = W_t + v_t
$$

- $W_{t+1}$: ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜
- $W_t$: í˜„ì¬ ë§¤ê°œë³€ìˆ˜
- $v_t$: í˜„ì¬ ì´ë™ ì†ë„

![Image](https://github.com/user-attachments/assets/26732553-b1dd-49cb-83f6-c2f9a520cf72){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

## 3. ğŸ”„ AdaGrad

> AdaGrad: ë§¤ê°œë³€ìˆ˜ì™€ ìŠ¤í…ë§ˆë‹¤ í•™ìŠµë¥ ì´ ë³€í•œë‹¤.

- í•™ìŠµë¥  ì¡°ì •ê°’ì˜ íŠ¹ì„±
  - ê° ì¶• ë³„ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •í•˜ëŠ” íš¨ê³¼
  - í° ë³€í™”ë¥¼ ê²ªì€ ë³€ìˆ˜ì˜ í•™ìŠµë¥ ì€ ëŒ€í­ ì‘ì•„ì§
  - ì‘ì€ ë³€í™”ë¥¼ ê²ªì€ ë³€ìˆ˜ì˜ í•™ìŠµë¥ ì€ ì†Œí­ ì‘ì•„ì§
  - ë‹¨ì : ë¬´í•œ í•™ìŠµì‹œ ê°±ì‹ ëŸ‰ì´ 0ì´ë˜ì–´ ì „í˜€ ê°±ì‹ ë˜ì§€ ì•ŠìŒ

$$
h = h + \frac{\partial L(W)}{\partial W} \odot \frac{\partial L(W)}{\partial W}
$$

- $h$: í•™ìŠµë¥  ì¡°ì •ê°’
- $\odot$: ìš”ì†Œ ê³±

$$
W_{t+1} = W_t - \frac{\eta}{\sqrt{h}} \frac{\partial L(W)}{\partial W}
$$

- $W_{t+1}$: ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜
- $W_t$: í˜„ì¬ ë§¤ê°œë³€ìˆ˜
- $\eta$: í•™ìŠµë¥ 
- $\sqrt{h}$: í•™ìŠµë¥  ì¡°ì •ê°’ì˜ ì œê³±ê·¼
- $\frac{\partial L(W)}{\partial W}$: ê¸°ìš¸ê¸°

![Image](https://github.com/user-attachments/assets/895bee2c-ff33-4472-93bd-595b4e10159b){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

## 4. â†—ï¸ NAG(Nesterov Accelerated Gradient)

> NAG: í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê´€ì„±ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ í›„(ëª¨ë©˜í…€) ìœ„ì¹˜ì—ì„œì˜ ê¸°ìš¸ê¸° ë°˜ëŒ€ ë°©í–¥ì„ í•©í•œë‹¤.

$$
v_t = \alpha v_{t-1} - \eta \nabla f(W + \alpha v_{t-1}) \quad where \quad v_{-1} = 0
$$

- $v_t$: ì´ë™ ì†ë„
- $\alpha$: ê´€ì„± ê³„ìˆ˜(ë°˜ì˜ë¥ )
- $\eta$: í•™ìŠµë¥ 
- $\nabla f(W)$: ê¸°ìš¸ê¸°

$$
W_{t+1} = W_t + v_t
$$

- $W_{t+1}$: ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜
- $W_t$: í˜„ì¬ ë§¤ê°œë³€ìˆ˜
- $v_t$: í˜„ì¬ ì´ë™ ì†ë„

### NAG vs ê¸°ì¡´ ëª¨ë©˜í…€

![Image](https://github.com/user-attachments/assets/7a94f8de-72c1-4550-bb64-be59bf8c8f1f){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: ìì²´ ì œì‘*

1. **ê¸°ì¡´ ëª¨ë©˜í…€**
   - í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
   - ê·¸ë˜ë””ì–¸íŠ¸ì™€ ì´ì „ ëª¨ë©˜í…€ì„ ê²°í•©
   - **ë³´ê³  ì´ë™í•˜ëŠ” ë°©ì‹**

2. **NAG**
   - ëª¨ë©˜í…€ ë°©í–¥ìœ¼ë¡œ ë¯¸ë¦¬ ì´ë™
   - ì˜ˆìƒ ìœ„ì¹˜ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
   - **ì´ë™í•˜ê³  ë³´ëŠ” ë°©ì‹**
   - ë” í˜„ëª…í•œ ë°©í–¥ ê²°ì • ê°€ëŠ¥

![Image](https://github.com/user-attachments/assets/adac02d7-b642-40ed-a68f-376a9fd47663){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

## 5. ğŸ“‰ RMSProp

> RMSProp: ê¸°ìš¸ê¸°ì˜ ì œê³±ì„ í‰ê· ë‚´ì–´ í•™ìŠµë¥ ì„ ì¡°ì •í•œë‹¤.

- AdaGradëŠ” ìŠ¤í…ì´ ë§ì´ ì§„í–‰ë˜ë©´ ëˆ„ì ì¹˜ hnì´ ë„ˆë¬´ ì»¤ì ¸ì„œ í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì•„ì ¸ í•™ìŠµì´ ê±°ì˜ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•œë‹¤.
- RMSPropì€ ì´ë¥¼ ë³´ì™„í•œ ë°©ë²•ìœ¼ë¡œ $\gamma$ë¥¼ ì¡°ì •í•˜ì—¬ í•™ìŠµë¥  ì¡°ì •ê°’ì˜ ê³¼ê±° ê°’ ë°˜ì˜ ë¹„ìœ¨ì„ ì¡°ì •í•œë‹¤.

$$
h_t = \gamma h_{t-1} + (1 - \gamma) \nabla f(W_t) \odot \nabla f(W_t) \quad where \quad h_{-1} = 0
$$

- $h_t$: í•™ìŠµë¥  ì¡°ì •ê°’
- $\gamma$: ê´€ì„± ê³„ìˆ˜(ë°˜ì˜ë¥ )
- $\nabla f(W_t)$: ê¸°ìš¸ê¸°

$$
W_{t+1} = W_t - \frac{\eta}{\sqrt{h_t}} \nabla f(W_t)
$$

- $W_{t+1}$: ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜
- $W_t$: í˜„ì¬ ë§¤ê°œë³€ìˆ˜
- $\eta$: í•™ìŠµë¥ 
- $\sqrt{h_t}$: í•™ìŠµë¥  ì¡°ì •ê°’ì˜ ì œê³±ê·¼

![Image](https://github.com/user-attachments/assets/5fcdf84e-27f7-454e-98dc-baca8a0403d6){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

## 6. ğŸš€ Adam(Adaptive Moment Estimation)

> Adam: ëª¨ë©˜í…€ê³¼ RMSPropì„ ê²°í•©í•œ ë°©ë²•

- ê²½ìš°ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” optimizerì´ë‹¤.
- í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ í¸í–¥ ë³´ì •(bias correction)ì´ ì§„í–‰ëœë‹¤.

$$
m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla f(W_t)
$$

- $m_t$: ëª¨ë©˜í…€
- $\beta_1$: ê´€ì„± ê³„ìˆ˜(ë°˜ì˜ë¥ )
- $\nabla f(W_t)$: ê¸°ìš¸ê¸°

$$
v_t = \beta_2 v_{t-1} + (1 - \beta_2) \nabla f(W_t) \odot \nabla f(W_t)
$$

- $v_t$: í•™ìŠµë¥  ì¡°ì •ê°’
- $\beta_2$: ê´€ì„± ê³„ìˆ˜(ë°˜ì˜ë¥ )

$$
W_{t+1} = W_t - \frac{\eta}{\sqrt{v_t}} \frac{m_t}{\sqrt{\epsilon}}
$$

- $W_{t+1}$: ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜
- $W_t$: í˜„ì¬ ë§¤ê°œë³€ìˆ˜
- $\eta$: í•™ìŠµë¥ 
- $\epsilon$: ì•ˆì •í™” ìƒìˆ˜

![Image](https://github.com/user-attachments/assets/9fba3882-9588-4296-bad6-41712397a5c5){: .align-center}

*ì´ë¯¸ì§€ ì¶œì²˜: [My Repository - MS AI School](https://github.com/GideokKim/ms-ai-school-study/tree/main/20250205)*

## 7. ğŸ“Š MNIST ë°ì´í„°ì…‹ìœ¼ë¡œ ë³¸ ë‹¤ì–‘í•œ optimizerì˜ ì„±ëŠ¥ ë¹„êµ

![Image](https://github.com/user-attachments/assets/9e7c9b35-d9ef-41f8-9a8f-01156be4773d){: .align-center}