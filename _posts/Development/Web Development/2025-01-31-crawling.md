---
title: "[Web Crawling] 웹 크롤링 기초 이론 정리"
date: 2025-01-31 10:00:00 +0900
last_modified_at: 2025-01-31 10:00:00 +0900
categories:
  - Web Development
tags:
  - Web Crawling
  - Web Scraping
  - python
  - selenium
  - beautifulsoup
  - robots.txt

toc: true
toc_sticky: true
---

# 웹 크롤링 개요

## 웹 크롤링 vs 웹 스크래핑

> 아래 두 기술이 함께 사용되어 웹 데이터를 수집한다.

### 웹 크롤링(Web Crawling)

- 자동화된 방식으로 웹페이지들을 순회하면서 링크를 따라 다니는 것
- 검색 엔진의 봇(예: Googlebot)이 대표적인 예시
- 주로 웹 사이트의 구조를 파악하고 새로운 페이지를 발견하는 것이 목적

### 웹 스크래핑(Web Scraping)

- 웹페이지에서 원하는 데이터를 추출하는 기술
- HTML 문서에서 특정 태그나 패턴을 찾아 정보를 수집
- 가격 비교, 뉴스 수집, 제품 정보 추출 등에 활용

## 크롤링 주의사항

### 법적 주의사항

- 웹사이트 콘텐츠는 대부분 저작권으로 보호됨
- 상당한 투자와 노력으로 구축된 데이터베이스는 제작자의 권리가 인정됨
- 무단 크롤링은 저작권법과 부정경쟁방지법 위반이 될 수 있음

### 기술적 주의사항

- 과도한 요청으로 서버에 부담을 주지 않도록 크롤링 간격 조정
  - **과도한 요청 시 영업방해로 인정될 수 있음**
- `robots.txt` 파일의 크롤링 규칙 확인 및 준수
- 기술적 제한을 우회하는 행위 금지

### 윤리적 고려사항

- 상업적 목적으로 사용 시 저작권자의 허가 필요
- 경쟁 업체의 데이터를 무단으로 활용하지 않도록 주의

### 대표적 소송 사례

- 야놀자 vs 여기어때 사건 (2023년)
  - 여기어때가 야놀자의 숙박업소 정보를 무단 크롤링하여 사용
  - 민사: 10억원 배상 판결 확정
  - 형사: 여기어때 무죄 판결
- 잡코리아 vs 사람인 사건
  - 사람인이 잡코리아의 채용정보를 크롤링하여 웹사이트에 게시
  - HTML 소스의 텍스트는 저작권 보호대상이 아니나, 데이터베이스 제작자 권리 인정
  - 상당한 투자와 노력으로 구축한 데이터베이스의 무단 크롤링은 부정경쟁행위로 판단

  ### `robots.txt` 확인 방법

  > 웹브라우저 주소창에 해당 웹사이트 주소 뒤에 `/robots.txt`를 입력하여 `robots.txt` 파일 다운로드

![Image](https://github.com/user-attachments/assets/754d0f42-caf9-4a70-9857-9fa3f1a2b87f){: .align-center}

- `User-agent` : 규칙을 적용할 크롤러
- `Disallow` : 접근 차단할 경로
- `Allow` : 접근 허용할 경로
- `Crawl-delay` : 크롤링 요청 간 대기 시간(초)
- `robots.txt`는 권고 사항이며 강제성은 없음
- 이 규칙을 준수하는 것은 웹 크롤링의 에티켓이나, 법정 분쟁 발생 시에는 고려 대상이 될 수 있음

## 크롤링을 위한 웹페이지의 유형

> 정적 페이지 크롤링과 동적 페이지 크롤링은 방법이 다르다. **정적 페이지 크롤링 방법으로는 동적 페이지 크롤링이 불가능하다.**

### 정적 페이지

> 한 번에 모든 HTML 문서를 응답 받는 페이지

- 파이썬 라이브러리 `requests` 사용
  - http 요청을 보내고 응답받음
  - 웹브라우저 열지 않음
- 파이썬 라이브러리 `BeautifulSoup` 사용
  -  html 문서 파싱, 데이터 추출

### 동적 페이지

> HTML을 응답 받은 후 일부 내용을 JavaScript가 동적으로 생성하는 페이지

- 파이썬 라이브러리 `Selenium` 사용
  - 웹브라우저를 열어 동적 콘텐츠 로드
  - 웹요소로부터 데이터 추출
  - html 문서 파싱, 데이터 추출

## 동적 페이지 크롤링

1. 웹드라이버 생성
2. 웹드라이버의 브라우저를 이용하여 목표 URL로 이동
3. 동적 콘텐츠 로딩
4. 필요 시 상호작용(클릭, 스크롤, 검색 등)
5. 데이터 추출
6. 데이터 저장
